name: Daily Scrape

on:
  schedule:
    # Run daily at 2:00 AM UTC
    - cron: "0 2 * * *"
  # Allow manual triggering of workflow
  workflow_dispatch:
    inputs:
      debug_enabled:
        description: "Run the workflow in debug mode"
        required: false
        default: false
        type: boolean

jobs:
  scrape:
    name: Run Daily Scraper
    runs-on: ubuntu-latest
    timeout-minutes: 120 # Set a timeout of 2 hours for the job

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Setup service account credentials
        run: |
          echo "${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}" > config/service-account.json
          echo "GOOGLE_SERVICE_ACCOUNT_FILE=config/service-account.json" >> .env

      - name: Setup environment variables
        run: |
          echo "GOOGLE_SHEETS_SPREADSHEET_ID=${{ secrets.GOOGLE_SHEETS_SPREADSHEET_ID }}" >> .env
          echo "GOOGLE_SHEETS_TITLE=LeadScraper Results $(date +'%Y-%m-%d')" >> .env
          echo "INSTAGRAM_USERNAME=${{ secrets.INSTAGRAM_USERNAME }}" >> .env
          echo "INSTAGRAM_PASSWORD=${{ secrets.INSTAGRAM_PASSWORD }}" >> .env
          echo "GOOGLE_MAPS_WAIT_TIME=5.0" >> .env  # Increase wait time in CI environment to avoid detection
          echo "HEADLESS_BROWSER=True" >> .env
          echo "LOG_LEVEL=INFO" >> .env

          # Optional: Configure additional environment variables from GitHub secrets
          if [ -n "${{ secrets.ADDITIONAL_ENV_VARS }}" ]; then
            echo "${{ secrets.ADDITIONAL_ENV_VARS }}" >> .env
          fi

      - name: Verify environment setup
        run: python check_env.py

      - name: Run scraper
        id: run_scraper
        continue-on-error: true # Continue to next steps even if this fails
        run: |
          mkdir -p logs
          mkdir -p results
          python main.py | tee scraper_output.log
          # Save exit code for later use
          echo "EXIT_CODE=$?" >> $GITHUB_ENV

      - name: Check for errors
        id: error_check
        run: |
          if [ "${{ env.EXIT_CODE }}" != "0" ]; then
            echo "Scraper completed with errors (exit code: ${{ env.EXIT_CODE }})"
            echo "scraper_status=failed" >> $GITHUB_ENV
          else
            echo "Scraper completed successfully"
            echo "scraper_status=success" >> $GITHUB_ENV
          fi

      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: scraper-results
          path: |
            results/
            logs/
            scraper_output.log
          retention-days: 7

      - name: Send success notification
        if: env.scraper_status == 'success'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "✅ [ScraperMVP] Daily scrape completed successfully"
          body: |
            The daily scrape job completed successfully.

            Timestamp: ${{ steps.date.outputs.date }}

            Check the attached files for results and logs.
          to: ${{ secrets.NOTIFICATION_EMAIL }}
          from: ScraperMVP <${{ secrets.SMTP_USERNAME }}>
          attachments: scraper_output.log

      - name: Send failure notification
        if: env.scraper_status != 'success'
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: ${{ secrets.SMTP_SERVER }}
          server_port: ${{ secrets.SMTP_PORT }}
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "❌ [ScraperMVP] Daily scrape failed"
          body: |
            The daily scrape job failed with exit code: ${{ env.EXIT_CODE }}

            Timestamp: ${{ steps.date.outputs.date }}

            Please check the attached log file for details.
          to: ${{ secrets.NOTIFICATION_EMAIL }}
          from: ScraperMVP <${{ secrets.SMTP_USERNAME }}>
          attachments: scraper_output.log

      # If scraper failed, exit with error code
      - name: Exit with error code if failed
        if: env.scraper_status != 'success'
        run: exit 1
